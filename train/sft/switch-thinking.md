# 思考模式切换

当前业界主流推理模型主要采用三种不同的技术路线来实现思考模式的切换：

#### 1 硬分叉

通过维护独立的模型权重来实现模式区分，简单来说，就是两个不同的模型，一个能够思考，一个不能。例如25年7月后，Qwen3被划分为两种模型：Qwen3-Instruct-2507 and Qwen3-Thinking-2507。

这两类模型一般会使用不同的监督微调（SFT）数据和强化学习（RL）策略分别训练。思考模型会强制模型输出由`<think>`标签包裹的推理链；而非思考模型则通过指令微调压制思考行为，直接生成最终答案。

#### 2 软切换

软切换机制允许同一模型通过提示词或参数动态切换行为模式。早期的Qwen3提供`/think`和`/no_think`指令配合`enable_thinking`参数；DeepSeek-V3.1则通过修改对话模板实现切换。

该路线依赖训练数据的混合策略：同时包含带`<think>`标签和不带标签的SFT数据，使模型学会根据指令选择生成模式。具体实现中常采用模板约束技术，例如通过预填充空的思考块强制模型跳过推理阶段：

```
messages = [
    {"role": "user", "content": "问题 /no_think"},
    {"role": "assistant", "content": "<think>\n\n</think>\n"}
]
```

这种方法利用Transformer的因果注意力机制，使模型基于已生成的空思考块判断"思考阶段已完成"，直接进入答案生成。\
软切换机制的优势在于部署成本（仅需加载一套权重）、多轮对话灵活性（支持逐轮切换思考模式），以及API兼容性（保持统一接口格式）。

#### 3 动态计算预算

动态计算预算架构在同一模型内通过参数控制测试时计算量（Test-Time Compute）。Claude-4通过API参数调节"thinking efforts"量级；Kimi k1.5则采用"Long2Short"技术，通过长思维链蒸馏实现可变深度推理。该路线的底层原理涉及强化学习对思考深度的控制：在RL阶段引入思考长度奖励，训练模型根据问题难度自适应调整推理步数。同时通过`max_tokens`、`temperature`或专门的`thinking_budget`参数物理限制推理token数量。Kimi k1.5的Long2Short技术进一步将长思维链知识蒸馏到短思维链，实现可伸缩推理。

这种设计追求认知一致性（避免能力割裂）、粒度控制（精细调节思考强度），以及效率优化（简单任务自动缩短思考长度，复杂任务自动扩展）。

#### 4 训练阶段的差异

不同模型在SFT和RL阶段植入思考行为的方式存在差异：

* DeepSeek-R1采用纯RL驱动策略，不提供思考过程数据，让模型自主探索`<think>`标签的使用方式
* Qwen3采用SFT混合数据加RLHF，人工构造带/不带思考标签的并行数据，显式教授模式切换指令
* Claude-4则采用可变长度RL，奖励函数同时考虑答案正确性和思考效率（以最少步骤获得正确答案）。

