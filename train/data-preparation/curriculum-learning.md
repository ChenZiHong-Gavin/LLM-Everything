# 课程学习

课程学习是一种训练策略，模仿人类的学习过程，主张让模型先从容易的样本开始学习，并逐渐进阶到复杂的样本和知识。

### 为什么做课程学习

传统的 LLM 训练方法往往依赖海量数据蒸馏（如DeepSeek-R1-Distill使用80万条SFT数据），或者盲目堆砌token数量，却忽视了模型学习的内在规律。很容易造成训练不稳定或过拟合。

论文: **Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and Beyond （**[https://arxiv.org/pdf/2503.10460](https://arxiv.org/pdf/2503.10460%EF%BC%89%E6%8F%90%E5%87%BA%E8%AE%A9%E9%82%A3%E4%BA%9B**%E5%8E%9F%E6%9C%AC%E4%B8%8D%E5%85%B7%E5%A4%87%E9%95%BFCoT%E8%83%BD%E5%8A%9B**%E7%9A%84%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%EF%BC%88%E5%A6%82Qwen2.5-32B-Instruct%EF%BC%89%EF%BC%8C%E9%80%9A%E8%BF%87%E6%B8%90%E8%BF%9B%E5%BC%8F%E9%9A%BE%E5%BA%A6%E6%8F%90%E5%8D%87%EF%BC%8C%E5%BE%97%E5%88%B0**%E6%8E%A8%E7%90%86%E8%83%BD%E5%8A%9B**%E7%9A%84%E6%8F%90%E5%8D%87**%E3%80%82**))，提出让那些**原本不具备长CoT能力**的基础模型（如Qwen2.5-32B-Instruct），通过渐进式难度提升，得到**推理能力**的提升。

### 怎么做课程学习

Light-R1 实施了一套多阶段后训练流程：

#### **Stage 1 SFT**

SFT 分为两阶段：

1. 使用**76,000条**数学推理数据建立基础CoT能力，重点是让模型掌握基本的思维链格式和简单推理模式。
2. 使用精心筛选的3,000条最具挑战性的题目，这3,000题与第一阶段的76,000题在知识结构上互补，专门填补模型的"逻辑盲区”，难度分布经过精心设计，确保模型在"舒适区边缘"持续成长。

#### Stage 2 **DPO**

在SFT之后，采用**半在线 DPO** 进行偏好优化：

* 利用验证后的响应对构建偏好数据
* 在不增加过高计算成本的前提下， refine模型的回答质量

#### Stage 3 GRPO

* **离线数据选择**：预先筛选通过率在0.25-0.625之间的"黄金难度"题目
  * 太简单（通过率>0.625）：长度崩溃
  * 太难（通过率<0.25）：训练不稳定
* **在线优化**：在筛选后的数据上进行GRPO训练，实现**响应长度和奖励分数的同步增长**

### 效果如何

在数学推理能力方面，Light-R1 系列模型展现出与更大规模模型相当或更优的测试成绩。同时，Light-R1 拥有了一定的跨领域泛化能力，尽管课程学习主要基于数学数据训练，Light-R1-14B-DS在GPQA（ graduate-level science benchmark）这类科学问答任务中也表现出相应的推理能力。这表明数学推理训练所产生的逻辑结构可能具有一定的领域迁移性。

在数据使用效率上，研究表明第二阶段使用的3000条精选难题对模型性能产生了积极影响。该数据集与第一阶段76000条数据及DeepSeek-R1-Distill使用的800000条数据在知识覆盖上存在差异，针对模型在基础训练后仍存在的特定推理缺陷进行了补充。实验观察到，这3000条数据在7B、14B和32B三种不同参数规模的模型上均带来了性能改善。
