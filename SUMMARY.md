# Table of contents

* [📃 前言](README.md)
* [🎚️ 基础部分](basics/README.md)
  * [🐍 Python基础](basics/python-basics/README.md)
    * [logging模块](basics/python-basics/logging.md)
    * [import模块](basics/python-basics/import.md)
    * [multiprocessing模块](basics/python-basics/multiprocessing.md)
  * [🐘 机器学习基础](basics/machine-learning-basics/README.md)
    * [特征提取](basics/machine-learning-basics/feature-extraction/README.md)
      * [文本表示模型](basics/machine-learning-basics/feature-extraction/text-representation-models/README.md)
        * [Bag-of-words](basics/machine-learning-basics/feature-extraction/text-representation-models/bag-of-words.md)
        * [Topic Model](basics/machine-learning-basics/feature-extraction/text-representation-models/topic-model.md)
        * [Static Word Embeddings](basics/machine-learning-basics/feature-extraction/text-representation-models/static-word-embeddings.md)
  * [🪿 深度学习基础](basics/deep-learning-basics.md)
  * [🐬 LLM基础](basics/llm-basics.md)
* [🦖 Transformer](transformer/README.md)
  * [Tokenizer](transformer/tokenizer.md)
  * [Embeddings](transformer/embeddings/README.md)
    * [ELMO](transformer/embeddings/elmo.md)
    * [BERT](transformer/embeddings/bert.md)
    * [GPT](transformer/embeddings/gpt.md)
  * [Positional Encoding](transformer/positional-encoding.md)
  * [Self Attention](transformer/self-attention.md)
  * [Multi-Head Attention](transformer/multi-head-attention.md)
  * [Add & Norm](transformer/add-and-norm.md)
  * [FeedForward](transformer/feedforward.md)
  * [Linear & Softmax](transformer/linear-and-softmax.md)
  * [Decoding Strategy](transformer/decoding-strategy.md)
* [Prompt Engineering](prompt-engineering/README.md)
  * [Tree of Thoughts](prompt-engineering/tree-of-thoughts.md)
* [LLM应用](llm-application/README.md)
  * [RAG](llm-application/rag/README.md)
    * [Graph RAG](llm-application/rag/graph-rag.md)
* [Linear Algebra](linear-algebra.md)
* [🎄 模型训练](train/README.md)
  * [LLM显存需求](train/llm-vram-needs/README.md)
    * [LLM精度问题](train/llm-vram-needs/llm-precision.md)
    * [LLM训练需要多少显存](train/llm-vram-needs/vram_needs_for_llm_training.md)
* [🐒 MoE](moe/README.md)
  * [专家并行](moe/expert-parallelism.md)
