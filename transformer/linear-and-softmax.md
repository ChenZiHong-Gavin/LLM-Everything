# Linear & Softmax

Transformer最终会为需要预测的下一个 token 生成一个高维向量。从高维向量到具体的词，例如“猫”、“学习”或者“天空”，需要经过 Linear & Softmax 层。

## 1 Linear

通过一次线性变换，将输入的上下文向量投影到词表空间。输出的长向量被称为 Logits，Logits中的每个元素都对应词汇表中的一个单词，其数值代表模型认为该单词是下一个正确单词的原始、未经归一化的置信度分数。分数越高，代表模型的信心越足。

## 2 Softmax

Logits分数虽然直观地表达了模型的偏好，但它们并不是标准的概率，存在两个问题：

1. 数值范围不固定，可以是任意正负实数。
2. 所有分数的总和不是1，无法直接用于概率计算或抽样。

Softmax的工作过程：

1. **指数化：** 通过指数函数 ($$ex$$) 将所有的Logits分数转换成正数。拉大高分和低分之间的差距，让模型的选择更加坚定。
2. **归一化：**&#x5C06;每个转换后的分数除以所有分数之和。

s



这时，Softmax函数就来解决这个问题。

* 功能定位： 它是一个归一化函数。
* 输入： 上一步线性层输出的整个Logits向量（例如，`[1, 50000]` 维）。
* 工作过程：
  1.
  2. &#x20;然后
* 输出结果： 输出一个同样维度（例如 `[1, 50000]`）的向量。这个向量是一个概率分布，其中：
  * 每个元素的值都在0到1之间。
  * 所有元素的总和恰好等于1。
  * 每个元素的值，代表了对应单词是下一个正确单词的最终预测概率。

继续比喻： Softmax函数接收了裁判（线性层）给出的所有选手的原始评分（Logits）。它首先通过指数运算将分数转化为一种“人气值”，然后计算出每个选手的人气值占总人气值的百分比。最终，我们得到了一份清晰的、关于每个选手获胜可能性的百分比报告。

***

#### 总结：两者如何协同工作

线性层 (Linear Layer) 和 Softmax 在输出端组成了一个不可分割的整体，完成了从模型内部的抽象思维到具体语言预测的最后一跃：

1. Transformer核心部分负责深度理解上下文，并生成一个最终的上下文向量。
2. 最后的线性层将这个抽象的上下文向量映射为对词汇表中每个单词的原始置信度分数 (Logits)。
3. Softmax函数再将这些分数转换为标准的概率分布，清晰地告诉我们模型预测下一个词是“A”的概率是 `x%`，是“B”的概率是 `y%`，以此类推。

有了这个最终的概率分布，模型就可以进行下一步操作了：

* 在推理（Inference）时： 可以选择概率最高的那个词（贪心搜索），或者根据概率分布进行抽样（例如在聊天机器人中生成更多样化的回答）。
* 在训练（Training）时： 可以将这个预测的概率分布与真实的下一个单词（One-Hot编码）进行比较，计算交叉熵损失（Cross-Entropy Loss），并据此反向传播更新整个模型的参数。





#### 第一步：线性层 (Linear Layer) —— 担当“解码词典”的角色

这个最后的线性层是连接模型内部抽象世界与人类语言具体词汇的关键桥梁。

* 功能定位： 它是一个投影层或映射层。
* 输入： 前面所有层计算出的最终高维上下文向量（例如，一个 `[1, 768]` 维的向量）。
* 输出维度： 这个线性层的特殊之处在于，它的输出维度被设定为整个词汇表的大小 (Vocabulary Size)。如果模型的词汇表包含5万个单词，那么这个线性层的输出就是一个 `[1, 50000]` 维的向量。
* 工作过程： 它通过一次线性变换，将输入的上下文向量“投影”到词汇表空间。这个变换的权重矩阵可以被理解为一个巨大的查找表，它学习了如何将一个富含上下文信息的“概念”映射到词汇表中每个具体单词的“可能性得分”上。
* 输出结果： 输出的这个长向量被称为 Logits。Logits向量中的每一个元素，都对应词汇表中的一个单词，其数值代表模型认为该单词是下一个正确单词的原始、未经归一化的置信度分数。分数越高，代表模型的信心越足。

简单比喻： 假设模型经过思考，形成了一个关于“一种毛茸茸的、会喵喵叫的宠物”的抽象概念（高维向量）。最后的这个线性层就像一本“概念-词汇”解码词典，它会查询这个概念，并为词汇表中的“猫”、“狗”、“桌子”等所有词打分。很显然，“猫”会得到一个非常高的分数，“狗”可能得到一个较低的正分，而“桌子”则可能是一个负分。

***

#### 第二步：Softmax 函数 —— 担当“概率转换器”的角色

Logits分数虽然直观地表达了模型的偏好，但它们并不是标准的概率，存在两个问题：

1. 数值范围不固定，可以是任意正负实数。
2. 所有分数的总和不是1，无法直接用于概率计算或抽样。

这时，Softmax函数就来解决这个问题。

* 功能定位： 它是一个归一化函数。
* 输入： 上一步线性层输出的整个Logits向量（例如，`[1, 50000]` 维）。
* 工作过程：
  1. 指数化： Softmax首先通过指数函数 ($$ex$$) 将所有的Logits分数转换成正数。这个操作会“拉大”高分和低分之间的差距，让模型的选择更加“坚定”。
  2. 归一化： 然后，将每个转换后的分数除以所有分数之和。
* 输出结果： 输出一个同样维度（例如 `[1, 50000]`）的向量。这个向量是一个概率分布，其中：
  * 每个元素的值都在0到1之间。
  * 所有元素的总和恰好等于1。
  * 每个元素的值，代表了对应单词是下一个正确单词的最终预测概率。

继续比喻： Softmax函数接收了裁判（线性层）给出的所有选手的原始评分（Logits）。它首先通过指数运算将分数转化为一种“人气值”，然后计算出每个选手的人气值占总人气值的百分比。最终，我们得到了一份清晰的、关于每个选手获胜可能性的百分比报告。

***

#### 总结：两者如何协同工作

线性层 (Linear Layer) 和 Softmax 在输出端组成了一个不可分割的整体，完成了从模型内部的抽象思维到具体语言预测的最后一跃：

1. Transformer核心部分负责深度理解上下文，并生成一个最终的上下文向量。
2. 最后的线性层将这个抽象的上下文向量映射为对词汇表中每个单词的原始置信度分数 (Logits)。
3. Softmax函数再将这些分数转换为标准的概率分布，清晰地告诉我们模型预测下一个词是“A”的概率是 `x%`，是“B”的概率是 `y%`，以此类推。

有了这个最终的概率分布，模型就可以进行下一步操作了：

* 在推理（Inference）时： 可以选择概率最高的那个词（贪心搜索），或者根据概率分布进行抽样（例如在聊天机器人中生成更多样化的回答）。
* 在训练（Training）时： 可以将这个预测的概率分布与真实的下一个单词（One-Hot编码）进行比较，计算交叉熵损失（Cross-Entropy Loss），并据此反向传播更新整个模型的参数。

## 参考

1. [https://blog.csdn.net/qq\_52099920/article/details/146389675](https://blog.csdn.net/qq_52099920/article/details/146389675)
2.
