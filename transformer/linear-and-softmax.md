# Linear & Softmax

Transformer最终会为需要预测的下一个 token 生成一个高维向量。从高维向量到具体的词，例如“猫”、“学习”或者“天空”，需要经过 Linear & Softmax 层。

## 1 Linear

通过一次线性变换，将输入的上下文向量投影到词表空间。输出的长向量被称为 Logits，Logits中的每个元素都对应词汇表中的一个单词，其数值代表模型认为该单词是下一个正确单词的原始、未经归一化的置信度分数。分数越高，代表模型的信心越足。

## 2 Softmax

Logits分数虽然直观地表达了模型的偏好，但它们并不是标准的概率，存在两个问题：

1. 数值范围不固定，可以是任意正负实数。
2. 所有分数的总和不是1，无法直接用于概率计算或抽样。

Softmax的工作过程：

1. **指数化：** 通过指数函数 ($$ex$$) 将所有的Logits分数转换成正数。拉大高分和低分之间的差距，让模型的选择更加坚定。
2. **归一化：**&#x5C06;每个转换后的分数除以所有分数之和。

经过了Softmax之后的元素的值，代表了对应单词是下一个正确单词的最终预测概率。

## 参考

1. [https://blog.csdn.net/qq\_52099920/article/details/146389675](https://blog.csdn.net/qq_52099920/article/details/146389675)
2.
