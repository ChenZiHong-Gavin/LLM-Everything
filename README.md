# ğŸ“ƒ å‰è¨€

<figure><img src=".gitbook/assets/Gemini_Generated_Image_nvoawnnvoawnnvoa.png" alt=""><figcaption></figcaption></figure>

[![GitBook](https://img.shields.io/static/v1?message=Documented%20on%20GitBook\&logo=gitbook\&logoColor=ffffff\&label=%20\&labelColor=5c5c5c\&color=3F89A1)](https://chenzihong.gitbook.io/llm-everything) [![çŸ¥ä¹](https://img.shields.io/static/v1?message=%E7%9F%A5%E4%B9%8E%E4%B8%93%E6%A0%8F\&logo=zhihu\&logoColor=ffffff\&label=%20\&labelColor=5c5c5c\&color=0084FF)](https://www.zhihu.com/column/c_1931824303218885390)

### ğŸŒŸåœ¨è¿™é‡Œå­¦ä¹ LLMï¼Œä½ å°†è·å¾—

* **è¶…ç”ŸåŠ¨å½¢è±¡çš„æŠ€æœ¯è®²è§£** ï¼šæˆ‘ä»¬æ‘’å¼ƒäº†é‚£äº›å……æ–¥ç€å¤§é‡ç®€å•å¤åˆ¶ç²˜è´´ä»¥åŠç”Ÿç¡¬ AI åˆæˆå†…å®¹çš„æŠ€æœ¯åšå®¢ï¼Œæ¯ä¸€ç¯‡æŠ€æœ¯æ–‡ç« éƒ½ç»è¿‡ç²¾å¿ƒæ‰“ç£¨ï¼Œè®©ä½ è½»æ¾ç†è§£å¤æ‚çš„çŸ¥è¯†ã€‚
* **è¶…æœ‰æ–™çš„æŠ€æœ¯å®æˆ˜** ï¼šä»é›¶å¼€å§‹ä¸€æ­¥æ­¥å®ç°ä»£ç ï¼Œå¸¦ä½ åœ¨å®æˆ˜ä¸­æ·±å…¥æ¢ç©¶åŸç†ï¼ŒçœŸæ­£æŒæ¡ LLM çš„ç²¾é«“ã€‚

### ğŸ“š LLMçŸ¥è¯†åœ°å›¾

* **åŸºç¡€éƒ¨åˆ†**
  * PythonåŸºç¡€
    * loggingæ¨¡å— ([logging.md](basics/python-basics/logging.md "mention"))
    * importæ¨¡å— ([import.md](basics/python-basics/import.md "mention"))
    * multiprocessingæ¨¡å— ([multiprocessing.md](basics/python-basics/multiprocessing.md "mention"))
  * æœºå™¨å­¦ä¹ åŸºç¡€
    * ç‰¹å¾æå–
      * æ–‡æœ¬è¡¨ç¤ºæ¨¡å‹
        * Bag-of-words ([bag-of-words.md](basics/machine-learning-basics/feature-extraction/text-representation-models/bag-of-words.md "mention"))
        * Topic Model ([topic-model.md](basics/machine-learning-basics/feature-extraction/text-representation-models/topic-model.md "mention"))
        * Static Word Embedding ([static-word-embeddings.md](basics/machine-learning-basics/feature-extraction/text-representation-models/static-word-embeddings.md "mention"))
  * æ·±åº¦å­¦ä¹ åŸºç¡€
  * LLMåŸºç¡€
* **Promptå·¥ç¨‹**
* **Transformerç»“æ„**
  * Tokenizer ([tokenizer.md](transformer/tokenizer.md "mention"))
  * Embedding ([embeddings](transformer/embeddings/ "mention"))
    * EMLO ([elmo.md](transformer/embeddings/elmo.md "mention"))
    * BERT ([bert.md](transformer/embeddings/bert.md "mention"))
    * GPT ([gpt.md](transformer/embeddings/gpt.md "mention"))
  * Positional Encoding ([positional-encoding.md](transformer/positional-encoding.md "mention"))
  * Self Attention ([self-attention.md](transformer/self-attention.md "mention"))
  * Multi-Head Attention ([multi-head-attention.md](transformer/multi-head-attention.md "mention"))
  * Add & Norm ([add-and-norm.md](transformer/add-and-norm.md "mention"))
  * FeedForward ([feedforward.md](transformer/feedforward.md "mention"))
  * Linear & Softmax ([linear-and-softmax.md](transformer/linear-and-softmax.md "mention"))
  * Decoding Strategy ([decoding-strategy.md](transformer/decoding-strategy.md "mention"))
* **LLMè®­ç»ƒ**
  * LLMæ˜¾å­˜éœ€æ±‚ ([llm-vram-needs](train/llm-vram-needs/ "mention"))
    * LLMç²¾åº¦é—®é¢˜ ([llm-precision.md](train/llm-vram-needs/llm-precision.md "mention"))
    * LLMè®­ç»ƒéœ€è¦å¤šå°‘æ˜¾å­˜ ([vram\_needs\_for\_llm\_training.md](train/llm-vram-needs/vram_needs_for_llm_training.md "mention"))
  * åˆ†å¸ƒå¼è®­ç»ƒå¹¶è¡Œ
    * æ•°æ®å¹¶è¡Œ ([data-parallelism.md](train/distributed-training-parallelism/data-parallelism.md "mention"))
    * æ¨¡å‹å¹¶è¡Œ ([model-parallelism.md](train/distributed-training-parallelism/model-parallelism.md "mention"))
    * ä¼˜åŒ–å™¨å¹¶è¡Œ ([optimizer-parallelism.md](train/distributed-training-parallelism/optimizer-parallelism.md "mention"))
* æ¨¡å‹æ¨ç†/éƒ¨ç½²
* MoE
  * ä¸“å®¶å¹¶è¡Œ ([expert-parallelism.md](moe/expert-parallelism.md "mention"))
* çŸ¥è¯†ç¼–è¾‘
* LLMåº”ç”¨
  * RAG
  * Graph RAG
* å¤šæ¨¡æ€å¤§æ¨¡å‹
* LLMå®‰å…¨

### ğŸ¤ ä¸ç¤¾åŒºä¸€èµ·æˆé•¿

æœ¬æ–‡æ¡£æ­£åœ¨å¿«é€Ÿè¿­ä»£ä¸­ã€‚\
åˆ†äº«ä½ çš„è§è§£ï¼Œæå‡ºä½ çš„ç–‘é—®ï¼Œå…±åŒè¿›æ­¥ã€‚
